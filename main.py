import os
import pandas as pd
from dotenv import load_dotenv
from sklearn.model_selection import train_test_split

from code.config import ExperimentConfig
from code.prompts.templates import TEMPLATES
from code.utils.experiment import ExperimentRunner

def main():
    # API í‚¤ ë¡œë“œ
    load_dotenv(dotenv_path=".env")
    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        raise ValueError("API key not found in environment variables")
    
    # ê¸°ë³¸ ì„¤ì • ìƒì„±
    base_config = ExperimentConfig(template_name='basic')
    
    # ë°ì´í„° ë¡œë“œ
    train = pd.read_csv(os.path.join(base_config.data_dir, 'train.csv'))
    test = pd.read_csv(os.path.join(base_config.data_dir, 'test.csv'))
    
    # í† ì´ ë°ì´í„°ì…‹ ìƒì„±
    toy_data = train.sample(n=base_config.toy_size, random_state=base_config.random_seed).reset_index(drop=True)
    
    # train/valid ë¶„í• 
    train_data, valid_data = train_test_split(
        toy_data,
        test_size=base_config.test_size,
        random_state=base_config.random_seed
    )
    
    # ëª¨ë“  í…œí”Œë¦¿ìœ¼ë¡œ ì‹¤í—˜
    results = {}
    for template_name in TEMPLATES.keys():
        config = ExperimentConfig(
            template_name=template_name,
            temperature=0.0,
            batch_size=5,
            experiment_name=f"toy_experiment_{template_name}"
        )
        runner = ExperimentRunner(config, api_key)
        results[template_name] = runner.run_template_experiment(train_data, valid_data)
    
    # ê²°ê³¼ ë¹„êµ
    print("\n=== í…œí”Œë¦¿ë³„ ì„±ëŠ¥ ë¹„êµ ===")
    for template_name, result in results.items():
        print(f"\n[{template_name} í…œí”Œë¦¿]")
        print("Train Recall:", f"{result['train_recall']['recall']:.2f}%")
        print("Train Precision:", f"{result['train_recall']['precision']:.2f}%")
        print("\nValid Recall:", f"{result['valid_recall']['recall']:.2f}%")
        print("Valid Precision:", f"{result['valid_recall']['precision']:.2f}%")
    
    # ìµœê³  ì„±ëŠ¥ í…œí”Œë¦¿ ì°¾ê¸°
    best_template = max(
        results.items(), 
        key=lambda x: x[1]['valid_recall']['recall']
    )[0]
    
    print(f"\nìµœê³  ì„±ëŠ¥ í…œí”Œë¦¿: {best_template}")
    print(f"Valid Recall: {results[best_template]['valid_recall']['recall']:.2f}%")
    print(f"Valid Precision: {results[best_template]['valid_recall']['precision']:.2f}%")
    
    # Recall 60% ì´ìƒì¸ ëª¨ë“  í…œí”Œë¦¿ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±
    print("\n=== í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘ ===")
    for template_name, result in results.items():
        valid_recall = result['valid_recall']['recall']
        if valid_recall >= 60.0:
            print(f"\nâœ… {template_name} í…œí”Œë¦¿ (Recall: {valid_recall:.2f}%) â†’ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
            config = ExperimentConfig(
                template_name=template_name,
                temperature=0.0,
                batch_size=5,
                experiment_name=f"final_submission_{template_name}"
            )
            runner = ExperimentRunner(config, api_key)
            test_results = runner.run(test)

            output = pd.DataFrame({
                'id': test['id'],
                'cor_sentence': test_results['cor_sentence']
            })
            output_filename = f"submission_{template_name}.csv"
            output.to_csv(output_filename, index=False)
            print(f"ğŸ“ ì €ì¥ ì™„ë£Œ: {output_filename}")

if __name__ == "__main__":
    main()
