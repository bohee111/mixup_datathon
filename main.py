"""
import os
import pandas as pd
from dotenv import load_dotenv
from sklearn.model_selection import train_test_split

from code.config import ExperimentConfig
from code.prompts.templates import TEMPLATES
from code.utils.experiment import ExperimentRunner

def main():
    # API í‚¤ ë¡œë“œ
    load_dotenv(dotenv_path=".env")
    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        raise ValueError("API key not found in environment variables")
    
    # ê¸°ë³¸ ì„¤ì • ìƒì„±
    base_config = ExperimentConfig(template_name='basic')
    
    # ë°ì´í„° ë¡œë“œ
    train = pd.read_csv(os.path.join(base_config.data_dir, 'train.csv'))
    test = pd.read_csv(os.path.join(base_config.data_dir, 'test.csv'))
    
    # í† ì´ ë°ì´í„°ì…‹ ìƒì„±
    toy_data = train.sample(n=base_config.toy_size, random_state=base_config.random_seed).reset_index(drop=True)
    
    # train/valid ë¶„í• 
    train_data, valid_data = train_test_split(
        toy_data,
        test_size=base_config.test_size,
        random_state=base_config.random_seed
    )
    
    # ëª¨ë“  í…œí”Œë¦¿ìœ¼ë¡œ ì‹¤í—˜
    results = {}
    for template_name in TEMPLATES.keys():
        config = ExperimentConfig(
            template_name=template_name,
            temperature=0.0,
            batch_size=5,
            experiment_name=f"toy_experiment_{template_name}"
        )
        runner = ExperimentRunner(config, api_key)
        results[template_name] = runner.run_template_experiment(train_data, valid_data)
    
    # ê²°ê³¼ ë¹„êµ
    print("\n=== í…œí”Œë¦¿ë³„ ì„±ëŠ¥ ë¹„êµ ===")
    for template_name, result in results.items():
        print(f"\n[{template_name} í…œí”Œë¦¿]")
        print("Train Recall:", f"{result['train_recall']['recall']:.2f}%")
        print("Train Precision:", f"{result['train_recall']['precision']:.2f}%")
        print("\nValid Recall:", f"{result['valid_recall']['recall']:.2f}%")
        print("Valid Precision:", f"{result['valid_recall']['precision']:.2f}%")
    
    # ìµœê³  ì„±ëŠ¥ í…œí”Œë¦¿ ì°¾ê¸°
    best_template = max(
        results.items(), 
        key=lambda x: x[1]['valid_recall']['recall']
    )[0]
    
    print(f"\nìµœê³  ì„±ëŠ¥ í…œí”Œë¦¿: {best_template}")
    print(f"Valid Recall: {results[best_template]['valid_recall']['recall']:.2f}%")
    print(f"Valid Precision: {results[best_template]['valid_recall']['precision']:.2f}%")
    
    # ìµœê³  ì„±ëŠ¥ í…œí”Œë¦¿ìœ¼ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±
    print("\n=== í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹œì‘ ===")
    config = ExperimentConfig(
        template_name=best_template,
        temperature=0.0,
        batch_size=5,
        experiment_name="final_submission"
    )
    
    runner = ExperimentRunner(config, api_key)
    test_results = runner.run(test)
    
    output = pd.DataFrame({
        'id': test['id'],
        'cor_sentence': test_results['cor_sentence']
    })
    
    output.to_csv("submission_baseline.csv", index=False)
    print("\nì œì¶œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: submission_baseline.csv")
    print(f"ì‚¬ìš©ëœ í…œí”Œë¦¿: {best_template}")
    print(f"ì˜ˆì¸¡ëœ ìƒ˜í”Œ ìˆ˜: {len(output)}")

if __name__ == "__main__":
    main()
"""

######################

import os
import pandas as pd
from dotenv import load_dotenv
from sklearn.model_selection import train_test_split

from config import ExperimentConfig
from templates import TEMPLATES
from experiment import ExperimentRunner
from code.config import ExperimentConfig
from metrics import find_differences_with_offsets

def main():
    # API í‚¤ ë¡œë“œ
    load_dotenv()
    api_key = os.getenv('UPSTAGE_API_KEY')
    if not api_key:
        raise ValueError("API key not found in .env í™˜ê²½ë³€ìˆ˜")

    # ì„¤ì • ë° runner ìƒì„±
    config = ExperimentConfig(template_name='basic')  # 'basic', 'detailed', 'formal' ì„ íƒ ê°€ëŠ¥
    runner = ExperimentRunner(config, api_key)

    # ë°ì´í„° ë¡œë”© ë° ë¶„í• 
    train = pd.read_csv(os.path.join(config.data_dir, 'train.csv'))
    train_data, valid_data = train_test_split(
        train.sample(n=config.toy_size, random_state=config.random_seed),
        test_size=config.test_size,
        random_state=config.random_seed
    )

    # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ Solar API ì˜ˆì¸¡ ìˆ˜í–‰
    valid_results = runner.run(valid_data)

    # êµì • ì‹¤íŒ¨í•œ ë¬¸ì¥ 30ê°œ ì €ì¥
    incorrect_examples = []
    for i in range(len(valid_data)):
        original = valid_data.iloc[i]['err_sentence']
        golden = valid_data.iloc[i]['cor_sentence']
        prediction = valid_results.iloc[i]['cor_sentence']

        gold_diff = find_differences_with_offsets(original, golden)
        pred_diff = find_differences_with_offsets(original, prediction)

        if gold_diff != pred_diff:
            incorrect_examples.append({
                "id": valid_data.iloc[i]['id'],
                "original": original,
                "golden": golden,
                "prediction": prediction
            })
        if len(incorrect_examples) >= 30:
            break

    # ì¶œë ¥
    print("\n=== Solar APIê°€ í‹€ë¦¬ê²Œ êµì •í•œ ê²€ì¦ ë¬¸ì¥ 30ê°œ ===")
    for idx, ex in enumerate(incorrect_examples, 1):
        print(f"\n[{idx}] ID: {ex['id']}")
        print("ğŸŸ¥ ì›ë¬¸:     ", ex["original"])
        print("âœ… ì •ë‹µ:     ", ex["golden"])
        print("ğŸ” êµì •ê²°ê³¼: ", ex["prediction"])

if __name__ == "__main__":
    main()
